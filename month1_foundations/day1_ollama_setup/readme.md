\# Day 1 - Ollama Local LLM Setup



\## What I built

A local LLM setup using Ollama running in Docker, 

called from Python.



\## Stack

\- Ollama (running in Docker)

\- Mistral 7B model

\- Python 3.13

\- Virtual environment



\## What I learned

\- How to run an LLM locally using Docker

\- How to call the LLM from Python

\- How system prompts work

\- How multi-turn conversation works



\## How to run

1\. Start Ollama container:

&nbsp;  docker start ollama



2\. Activate virtual environment:

&nbsp;  venv\\Scripts\\activate



3\. Install dependencies:

&nbsp;  pip install -r requirements.txt



4\. Run:

&nbsp;  python test\_ollama.py

